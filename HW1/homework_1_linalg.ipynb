{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:**Haitham Khedr**  \n",
    "UID:**116098700**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1:  Linear Algebra \n",
    "### Due Feb 6, 2018\n",
    "Upload **both** your notebook (a file with extension \"ipynb\") **and** a pdf version of your notebook to [ELMS](https://umd.instructure.com/courses/1255897).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:  Dual norm\n",
    "\n",
    "Given some norm $\\|\\cdot\\|,$ it's dual norm $\\|\\cdot\\|_*$ is defined as\n",
    "$$\\|x\\|_*  = \\max_{\\|z\\|\\le 1} z^Tx.$$\n",
    "Prove the dual norm is indeed a norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "####  Homogeneity \n",
    "\n",
    "$||ax||_* = \\max_{\\|z\\|\\le 1}z^Tax = |a|\\max_{\\|z\\|\\le 1}z^Tx = |a|||x||_*, \\forall a \\in \\mathbb{R}$\n",
    "\n",
    "Note that if $a < 0$ we can choose $-z$ instead of $z$ to make $||ax||_* > 0$, that's why $|a|$ appeared \n",
    "\n",
    "#### Sub-additivity \n",
    "\n",
    "$||x + y||_* = \\max_{\\|z\\|\\le 1}z^T(x+y) \\leq \\max_{\\|z\\|\\le 1}z^Tx + \\max_{\\|z\\|\\le 1} z^Ty = ||x ||_* + ||y||_* \\\\ \\therefore ||x + y||_* \\leq ||x ||_* + ||y||_*$\n",
    "\n",
    "#### Separability: \n",
    "\n",
    "1-if $x$ = **0**, it is clear that $||x||_* =0$\n",
    "\n",
    "2-if $x \\neq$ **0**, we can always find $z$ s.t $||x||_* > 0$, ie. $z = \\frac{x}{||x||}$ then $||x||_* = \\max_{\\|z\\|\\le 1}z^Tx =\\frac{||x||^2}{||x||} = ||x|| \\neq 0$ unless $x$ = **0**\n",
    "\n",
    "From 1 & 2 $||x||_* = 0$ iff. $x$ = **0**\n",
    "\n",
    "Therefore, it is shown that the dual norm satisfies the three neceessary properties of a norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Consider two discrete sequences $f=(f_0,f_1,f_2,\\ldots, f_N),\\, g=(g_0,g_1,g_2,\\ldots, g_N).$  The sequences have period $N$, meaning that $f[s+N] = f[s-N]= f[s]$ for all $s\\in\\mathbb{Z}.$  We can combine these two functions into a new function by multiplying ($\\cdot$), filtering ($\\square$), or convolving ($*$).\n",
    "\n",
    "We can define the new functions produced by these operations coordinate-wise.  The three functions we are interested in are... \n",
    "\n",
    "**Coordinate-wise multiply:**  $(f\\cdot g)[s] = f[s]g[s].$\n",
    "\n",
    "**Linear Filter**:  $(f\\square g)[s] =  \\sum_{t=0}^{N-1} f(s+t)g(t)$\n",
    "\n",
    "**Convolution**:  $(f*g)[s] =  \\sum_{t=0}^{N-1} f(s-t)g(t)=  \\sum_{t=0}^N f(s+t)g(-t)$\n",
    "\n",
    "Note that convolving $f(\\cdot)$ with $g(\\cdot)$ is the same as filtering $f(\\cdot)$ with the \"flipped\" function $g(- \\cdot).$\n",
    "\n",
    "Given an $N$-periodic function $f$, it's discrete Fourier transform $\\hat f$ is defined by \n",
    "  $$\\hat f [u] = \\sum_{s=0}^{N-1} \\omega^{us} f[s].$$\n",
    "\n",
    "**Prove the Convolution Theorem:**  The Fourier transform of $f*g$ is the coordinate-wise product of $\\hat f$ and $\\hat g.$  In other words\n",
    "  $$\\widehat{f*g} = \\hat f \\cdot \\hat g.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "Let $h[n] = {f[n]*g[n]} = \\sum_{s=0}^{N-1} f[n-s]g[s]$, and $\\hat H[k] = \\widehat{f*g}$\n",
    "\n",
    "$\\hat H[k] = \\sum_{n=0}^{N-1} (f[n]*g[n]) \\omega^{kn} = \\sum_{n=0}^{N-1}\\sum_{s=0}^{N-1} f[n-s]g[s]\\omega^{kn} = \\sum_{s=0}^{N-1}g[s]\\sum_{n=0}^{N-1} f[n-s]\\omega^{kn}$\n",
    "\n",
    "\n",
    "$\\hat H[k] = \\sum_{s=0}^{N-1}g[s]\\sum_{m=-s}^{N-1-s} f[m]\\omega^{k(m+s)}=\\sum_{s=0}^{N-1}g[s]\\omega^{ks}\\sum_{m=0}^{N-1} f[m]\\omega^{km} = \\hat F[k] \\cdot \\hat G[k]$\n",
    "$$\\therefore \\hat H[k] = \\widehat{f*g} =   \\hat F[k] \\cdot \\hat G[k]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:  Fourier matrix\n",
    "The discrete Fourier transform of a signal is a linear operator that can be performed very quickly using the fast Fourier transform (FFT).  Because it's a linear operator, we can represent the discrete Fourier transform as a matrix. \n",
    "An $N\\times N$ Fourier matrix has entries\n",
    "   $$F_N[r,c] = \\frac{\\omega^{rc}}{\\sqrt{N}}, \\text{ for } 0\\le r,c \\le N-1.$$\n",
    "where $\\omega_{N} = \\exp\\left(\\frac{-2\\pi i}{N}\\right)$ is the Nth root of unity.  This number has the property that $\\omega_N^0 = \\omega_N^N = 1,$ and $|\\omega_N|=1.$  A Fourier matrix is Hermitian symmetric ($F_N^H=F_N$), orthogonal ($F^H_NF_N=I_N$), and looks kinda like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "F = np.fft.fft(np.eye(100))  # Create a matrix representation by invoking the FFT on the identity.\n",
    "plt.imshow(np.real(F));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest version of the fast Fourier transform computes the DFT quickly using the Cooley-Tukey factorization\n",
    " $$F_N x = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc} I & D \\\\ I & -D \\end{array}\\right)  \\left(\\begin{array}{c}  F_{N/2}x_e \\\\   F_{N/2}x_o \\end{array}\\right)\n",
    "$$\n",
    " where $D_{kk} = exp(-2\\pi i k/N)$ is a diagonal matrix of \"twiddle factors,\" $x_e=(x_0,x_2,x_4,\\cdots)$ contains even-indexed components of $x$, and $x_o$ contains the odd-indexed components.  If $N$ is a power of 2, then this reduction formula enables us to compute the product $F_N x$ in $O(N\\log N)$ time, which is MUCH faster than the naive matrix multiplication algorithm, which is $O(N^2).$ \n",
    " \n",
    "**Prove the Cooley-Tukey factorization formula.**\n",
    " \n",
    " Hint:  Use this simpler/algebraic statement of the theorem:\n",
    "   $$\\hat x[s] = \\frac{1}{\\sqrt{2}} \\big(\\hat x_e[s]+ \\exp(-2\\pi i s/N) \\hat x_o[s] \\big). $$\n",
    " Here, the sequences $\\hat x_e$ and $\\hat x_o[s]$ are interpreted to be symmetric, i.e., $\\hat x_e[i+N/2] = \\hat x_e[n].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "By definition (using the matrix form $F_N[r,c]$ provided above), the normalized DFT of a discrete-time signal $x[n]$ is given by $\\hat x[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1}x[n]e^{i\\frac{-2\\pi k}{N}n} = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1}x[n]\\omega_N^{kn}$\n",
    "\n",
    "We can split this sum to be over even and odd entries of the signal to be $\\hat x[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{\\frac{N}{2}-1}x[2n]\\omega_N^{2kn} + \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{\\frac{N}{2}-1}x[2n + 1]\\omega_N^{k(2n + 1)} = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{\\frac{N}{2}-1}x[2n]\\omega_N^{2kn} + \\frac{1}{\\sqrt{N}} \\omega^k \\sum_{n=0}^{\\frac{N}{2}-1}x[2n + 1]\\omega_N^{2kn}$\n",
    "\n",
    "We also have that $\\omega_N^{2kn} = e^{i\\frac{-2\\pi k}{N}2n} = e^{i\\frac{-2\\pi k}{N/2}n}= \\omega_{\\frac{N}{2}}^{kn}$, by substituting back into the sum we get\n",
    "$\\hat x[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{\\frac{N}{2}-1}x[2n]\\omega_{N/2}^{kn} + \\frac{1}{\\sqrt{N}} \\omega^k \\sum_{n=0}^{\\frac{N}{2}-1}x[2n + 1]\\omega_{N/2}^{kn}$\n",
    "\n",
    "$\\hat x[k] = \\frac{1}{\\sqrt{2}}(\\frac{1}{\\sqrt{\\frac{N}{2}}} \\sum_{n=0}^{\\frac{N}{2}-1}x[2n]\\omega_{N/2}^{kn}) + \\frac{1}{\\sqrt{2}}\\omega^k (\\frac{1}{\\sqrt{\\frac{N}{2}}}\\sum_{n=0}^{\\frac{N}{2}-1}x[2n + 1]\\omega_{N/2}^{kn})$\n",
    "\n",
    "We can see that the first sum is the normalized DFT of the even part of the signal with length N/2 and the second sum is the normalized DFT of the odd part of the signal with length N/2\n",
    "$$\\therefore    \\hat x[k] = \\frac{1}{\\sqrt{2}}(\\hat x_e[k]+ e^{-i\\frac{2\\pi k}{N}} \\hat x_o[k]).$$which can be written in Matrix form as given above, noting that the negative sign in the 2nd row comes from the fact that $\\omega^k$ is negative for $k \\geq \\frac{N}{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Consider the measurement model\n",
    "  $$y = Dx + \\eta$$\n",
    "  where $D\\in m\\times n$ is a measurement matrix, and $\\eta\\in \\mathbb{R}^N$ is a noise vector drawn from a Gaussian distribution with covariance matrix $\\Sigma.$ The density function for this distribution is\n",
    "  $$ \\eta \\sim \\frac{1}{\\sqrt{(2\\pi)^m|\\Sigma|}}\\exp(- \\frac{1}{2}\\eta^T \\Sigma^{-1}\\eta) $$\n",
    "   where $|\\Sigma|$ denotes the determinant of $\\Sigma.$  Suppose we have prior knowledge that every entry in $x$ is draw from an i.i.d. Laplace distribution \n",
    "  $$x_i \\sim \\frac{1}{2b}\\exp(-|x|/b).$$\n",
    "  \n",
    "  Derive the negative log-likelihood function for $x$ given (fixed) measurements $y$.  Write the complete NLL without throwing away any constants (although you may use Bayes' rule, which implicitly throws away a normalization constant).   \n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Let $x \\in\\mathbb{R^n}$\n",
    "\n",
    "$p(x) = \\prod_{n=0}^{N}\\frac{1}{2b}\\exp(\\frac{-|x_n|}{b}) = \\frac{1}{(2b)^N}\\exp(\\sum_{n=0}^{N}\\frac{-|x_n|}{b}) = \n",
    "\\frac{1}{(2b)^N}\\exp(\\frac{-||x||_1}{b})$\n",
    "\n",
    "$p(y|x) \\sim N(0,\\Sigma)=\\frac{1}{\\sqrt{(2\\pi)^m|\\Sigma|}}\\exp(- \\frac{1}{2}\\eta^T \\Sigma^{-1}\\eta)$\n",
    "\n",
    "We use Bayes rule to compute the density of x|y as follows\n",
    "\n",
    "$p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$, where $p(y)$ is just a normalization constant(which is going to be dropped)\n",
    "\n",
    "$p(x|y) \\sim p(y|x)p(x) = \\frac{1}{\\sqrt{(2\\pi)^m|\\Sigma|}}\\exp(- \\frac{1}{2}\\eta^T \\Sigma^{-1}\\eta)\\frac{1}{(2b)^N}\\exp(\\frac{-||x||_1}{b})$\n",
    "\n",
    "The negative Log-likelihood function is derived by taking the -log(natural log) of  $p(x|y)$ which gives\n",
    "\n",
    "$L = log[(2b)^N\\sqrt{(2\\pi)^m|\\Sigma|}] +\\frac{1}{2}\\eta^T\\Sigma^-1\\eta + \\frac{||x||_1}{b}$\n",
    "\n",
    "$L = Nlog(2b) + \\frac{m}{2}log(2\\pi) + \\frac{1}{2}log(|\\Sigma|) + \\frac{1}{2}\\eta^T\\Sigma^{-1}\\eta + \\frac{||x||_1}{b}$\n",
    "\n",
    "$L = Nlog(2b) + \\frac{m}{2}log(2\\pi) + \\frac{1}{2}log(|\\Sigma|) + \\frac{1}{2}(y-Dx)^T\\Sigma^{-1}(y-DX) + \\frac{||x||_1}{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NOW, throw away all constants that don't affect the solution, and write a minimization problem for finding the maximum a posteriori (MAP) estimator  (this final answer should look quite simple).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "The maximum a posteriori (MAP) estimator is a maximization of posterior density function (or equivalently the minimization of the NLL function) as follows\n",
    "\n",
    "${\\text{min}}$  $L$ $= {\\text{min}}$ $\\frac{1}{2}(y-Dx)^T\\Sigma^{-1}(y-DX) + \\frac{||x||_1}{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
